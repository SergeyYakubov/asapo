job "asapo-logging" {
  datacenters = ["dc1"]

#  update {
#    max_parallel = 1
#    min_healthy_time = "10s"
#    healthy_deadline = "3m"
#    auto_revert = false
#  }

  group "fluentd" {
    count = 1
    restart {
      attempts = 2
      interval = "3m"
      delay = "15s"
      mode = "delay"
    }

    task "fluentd" {
      driver = "docker"

      config {
        dns_servers = ["127.0.0.1"]
        network_mode = "host"
        image = "yakser/fluentd_elastic"
        volumes = ["local/fluentd.conf:/fluentd/etc/fluent.conf",
        "/${meta.shared_storage}/fluentd:/shared"]
      }

      resources {
        cpu    = 500
        memory = 256
        network {
          mbits = 10
          port "fluentd" {
          static = 9880
          }
        }
      }

      service {
        port = "fluentd"
        name = "fluentd"
        check {
          name     = "alive"
          type     = "script"
          command  = "/bin/pidof"
          args     = ["ruby2.3"]
          timeout  = "2s"
	      interval = "10s"
        }
        check_restart {
          limit = 2
          grace = "15s"
          ignore_warnings = false
        }
      }
      template {
         source        = "@NOMAD_INSTALL@/fluentd.conf"
         destination   = "local/fluentd.conf"
         change_mode   = "restart"
      }
   }
  }
#elasticsearch
  group "elk" {
    count = 1
    restart {
      attempts = 2
      interval = "3m"
      delay = "15s"
      mode = "delay"
    }

    task "elasticsearch" {
      driver = "docker"

      env {
        bootstrap.memory_lock = "true"
        cluster.name = "asapo-logging"
        ES_JAVA_OPTS = "-Xms512m -Xmx512m"
      }

      config {
        ulimit {
          memlock = "-1:-1"
          nofile = "65536:65536"
          nproc = "8192"
        }
        network_mode = "host"
        dns_servers = ["127.0.0.1"]
        image = "docker.elastic.co/elasticsearch/elasticsearch:6.3.0"
        volumes = ["/${meta.shared_storage}/esdatadir:/usr/share/elasticsearch/data"]
      }

      resources {
        #MHz
        cpu = 4000
        #MB
        memory = 2048
        network {
          mbits = 10
          port "elasticsearch" {
            static = 9200
          }
         }
      }

      service {
        port = "elasticsearch"
        name = "elasticsearch"
        check {
            name = "alive"
            type     = "http"
	        path     = "/_cluster/health"
            interval = "10s"
            timeout  = "1s"
        }
        check_restart {
          limit = 2
          grace = "90s"
          ignore_warnings = false
        }
      }
   }
#kibana
   task "kibana" {
     driver = "docker"

     config {
       network_mode = "host"
       dns_servers = ["127.0.0.1"]
       image = "docker.elastic.co/kibana/kibana:6.3.0"
       volumes = ["local/kibana.yml:/usr/share/kibana/config/kibana.yml"]
     }

      template {
         source        = "@NOMAD_INSTALL@/kibana.yml"
         destination   = "local/kibana.yml"
         change_mode   = "restart"
      }

     resources {
       cpu = 256
       memory = 1024
       network {
         mbits = 10
         port "kibana" {
           static = 5601
         }
        }
     }

     service {
       port = "kibana"
       name = "kibana"
       check {
           name = "alive"
           type     = "http"
           path     = "/logsview"
           interval = "10s"
           timeout  = "1s"
       }
       check_restart {
         limit = 2
         grace = "90s"
         ignore_warnings = false
       }
     }
  }

  }

}
